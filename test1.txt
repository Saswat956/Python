import numpy as np
from sklearn.metrics import confusion_matrix, roc_curve, auc

class MetricsCalculator:

    def __init__(self, data):
        self.data = data

    def calculate_metrics(self):
        roc_dict_train = {}
        age_grps = list(self.data.select('age_brackets').drop_duplicates().toPandas()["age_brackets"].values)

        for age_ in age_grps:
            ly = np.array(self.data.filter(F.col('age_brackets') == age_).select('binary_label').toPandas()['binary_label'])
            flag = np.array(self.data.filter(F.col('age_brackets') == age_).select('flag_multipler').toPandas()['flag_multipler'])
            tn, fp, fn, tp = confusion_matrix(ly, flag).ravel()

            # Calculate ROC curve
            fpr, tpr, _ = roc_curve(ly, flag)

            # Calculate ROC AUC score
            roc_auc = auc(fpr, tpr)
            Precision = tp / (tp + fp)
            Recall = tp / (tp + fn)

            roc_dict_train[age_ + '_precision'] = round(Precision, 2)
            roc_dict_train[age_ + '_recall'] = round(Recall, 2)
            roc_dict_train[age_ + '_roc_auc'] = roc_auc
            roc_dict_train[age_ + '_fpr'] = fpr
            roc_dict_train[age_ + '_tpr'] = tpr

        ly = np.array(self.data.select('binary_label').toPandas()['binary_label'])
        flag = np.array(self.data.select('flag_multipler').toPandas()['flag_multipler'])
        tn, fp, fn, tp = confusion_matrix(ly, flag).ravel()

        Precision = tp / (tp + fp)
        Recall = tp / (tp + fn)

        roc_dict_train['overall_precision'] = round(Precision, 2)
        roc_dict_train['overall_recall'] = round(Recall, 2)
        roc_dict_train['overall_roc_auc'] = auc(fpr, tpr)
        roc_dict_train['overall_fpr'] = fpr
        roc_dict_train['overall_tpr'] = tpr

        return roc_dict_train

    @staticmethod
    def create_metrics_dataframe(metrics_dict):
        df = pd.DataFrame(metrics_dict.items(), columns=['Metric', 'Value'])
        return df







