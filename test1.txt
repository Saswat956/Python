from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder.appName("CategoricalDownsampling").getOrCreate()

# Assuming you have a DataFrame named 'df' with the columns you mentioned

# Define the fractions for downsampling
fractions = {
    ('age_brackets_value1', 'cat_rndrg_prov_txnmy_cd_value1', 'binary_label_value1'): 0.2,
    ('age_brackets_value1', 'cat_rndrg_prov_txnmy_cd_value1', 'binary_label_value2'): 0.5,
    # Define more fractions for other combinations as needed
}

# Calculate the complementary fractions (1 - fraction) for the remaining combinations
complementary_fractions = {
    combination: 1.0 - fraction for combination, fraction in fractions.items()
}

# Combine fractions and complementary fractions into a single dictionary
combined_fractions = fractions.copy()
combined_fractions.update(complementary_fractions)

# Use the sampleBy method to downsample based on the combined fractions
sampled_df = df.sampleBy('age_brackets', 'cat_rndrg_prov_txnmy_cd', 'binary_label', fractions=combined_fractions)

# Show the sampled DataFrame
sampled_df.show()

# Stop the Spark session
spark.stop()
